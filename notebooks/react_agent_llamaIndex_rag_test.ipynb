{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ba0126-56d3-426e-977f-aee8a94646a6",
   "metadata": {},
   "source": [
    "# ReAct agent Pattern - LlamaIndex RAG Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d120da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "%pip install llama-index\n",
    "%pip install llama-index-agent-openai  # For ReActAgent\n",
    "%pip install llama-index-llms-huggingface-api\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-core\n",
    "%pip install llama-index-llms-groq\n",
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dc83a-f11b-469d-bb2c-afbd91f39c5e",
   "metadata": {},
   "source": [
    "## Relevant imports and Groq Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605420e2-4bab-4d0a-90ac-7b9a95fd9976",
   "metadata": {},
   "source": [
    "We start by importing all the libraries we'll be using in this tutorial as well as the Groq client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056fc0ed-7ded-490b-ae0b-e7d1fd71430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "# LlamaIndex core imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.agent import ReActAgent, AgentWorkflow\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# ChromaDB for vector storage\n",
    "import chromadb\n",
    "\n",
    "#from groq import Groq\n",
    "\n",
    "from agentic_patterns.tool_pattern.tool import tool\n",
    "from agentic_patterns.utils.extraction import extract_tag_content\n",
    "\n",
    "\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "#GROQ_CLIENT = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4363c-b6e0-4c4a-aa08-ad243ddf7911",
   "metadata": {},
   "source": [
    "> If you are not familiar with the `tool` decorator, chances are you are missed the previous tutorial about the Tool Pattern. Check the video [here](https://www.youtube.com/watch?v=ApoDzZP8_ck&t=671s&ab_channel=TheNeuralMaze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649ab782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Project Root: ..\n",
      "ðŸ’¾ Vector DB Directory: ../data/vector_db\n",
      "ðŸ“„ Sample Data Directory: ../resources/sample-datasets\n",
      "\n",
      "âœ… Paths configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "VECTOR_DB_DIR = PROJECT_ROOT / \"data\" / \"vector_db\"\n",
    "SAMPLE_DATA_DIR = PROJECT_ROOT / \"resources\" / \"sample-datasets\"\n",
    "\n",
    "# Create necessary directories\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"ðŸ’¾ Vector DB Directory: {VECTOR_DB_DIR}\")\n",
    "print(f\"ðŸ“„ Sample Data Directory: {SAMPLE_DATA_DIR}\")\n",
    "print(f\"\\nâœ… Paths configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1dcfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LlamaIndex settings configured:\n",
      "   - Embedding Model: BAAI/bge-small-en-v1.5\n",
      "   - LLM: Groq llama-3.3-70b-versatile\n",
      "   - Chunk Size: 512\n",
      "   - Chunk Overlap: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # Lightweight, high-quality embedding model\n",
    "    cache_folder=str(PROJECT_ROOT / \"models\")\n",
    ")\n",
    "\n",
    "# You can specify the model you want to use, e.g., \"llama-3.3-70b-versatile\"\n",
    "# If you don't specify a model, it defaults to \"mixtral-8x7b-32768\"\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ… LlamaIndex settings configured:\")\n",
    "print(f\"   - Embedding Model: BAAI/bge-small-en-v1.5\")\n",
    "print(f\"   - LLM: Groq llama-3.3-70b-versatile\")\n",
    "print(f\"   - Chunk Size: 512\")\n",
    "print(f\"   - Chunk Overlap: 50\")\n",
    "\n",
    "# Now you can use it in your queries\n",
    "#response = llm.complete(\"What is the distance between the Earth and the Moon?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0262f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading existing vector index from Phase 1...\n",
      "âœ… Found existing collection: internal_knowledge_base\n",
      "   Total vectors: 3\n",
      "âœ… Vector index loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load existing vector index from Phase 1\n",
    "print(\"ðŸ”„ Loading existing vector index from Phase 1...\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=str(VECTOR_DB_DIR))\n",
    "collection_name = \"internal_knowledge_base\"\n",
    "\n",
    "# Load the collection\n",
    "try:\n",
    "    chroma_collection = chroma_client.get_collection(name=collection_name)\n",
    "    print(f\"âœ… Found existing collection: {collection_name}\")\n",
    "    print(f\"   Total vectors: {chroma_collection.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading collection: {e}\")\n",
    "    print(\"   Please run Phase 1 notebook first to create the vector index.\")\n",
    "    raise\n",
    "\n",
    "# Create ChromaVectorStore wrapper\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=str(VECTOR_DB_DIR))\n",
    "\n",
    "# Load the index\n",
    "try:\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    print(\"âœ… Vector index loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading index: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd00354-7ebc-44ec-9606-0a178af59b44",
   "metadata": {},
   "source": [
    "### Defining the Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6360cd-b9f2-4427-91bc-27f010147c04",
   "metadata": {},
   "source": [
    "Let's build an RAG tool that involves the use of a rag_query_engine tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d3bec1-679d-4d80-bf41-bf54b00e985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up QueryEngine Tool...\n"
     ]
    }
   ],
   "source": [
    "# Create QueryEngineTool from the vector index\n",
    "print(\"ðŸ”§ Setting up QueryEngine Tool...\")\n",
    "\n",
    "# Create query engine with optimized settings for agent use\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,      # Retrieve top 5 most similar chunks\n",
    "    response_mode=\"compact\", # Concatenate chunks and generate single response\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "@tool\n",
    "def rag_query_engine(query: str) -> str:\n",
    "    \"\"\"\n",
    "    A tool to query the knowledge base containing internal company documents. \n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search the knowledge base.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the knowledge base along with citation sources.\n",
    "        example: According to our HR policies, parental leave is 16 weeks. [Sources: company_handbook.md].\n",
    "    \"\"\"\n",
    "\n",
    "    citations = []\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    if response and hasattr(response, \"source_nodes\"):\n",
    "        citations = [node.node.metadata.get('file_name', 'Unknown') for node in response.source_nodes]\n",
    "\n",
    "    final_answer = str(response) + f\"\\n\\n[Sources: {', '.join(citations)}\" + \"]\"\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "available_tools = {\n",
    "    \"rag_query_engine\": rag_query_engine\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3044e-5d5c-44d0-973b-0c7c31f9724b",
   "metadata": {},
   "source": [
    "Remember that the `@tool` operator allows us to convert a Python function into a `Tool` automatically. We cana check that very easily with some of the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7e2078-9c5d-4687-8c1b-56b36a0194db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name:  rag_query_engine\n",
      "Tool signature:  {\"name\": \"rag_query_engine\", \"description\": \"\\n    A tool to query the knowledge base containing internal company documents. \\n\\n    Args:\\n        query (str): The query string to search the knowledge base.\\n\\n    Returns:\\n        str: The response from the knowledge base along with citation sources.\\n        example: According to our HR policies, parental leave is 16 weeks. [Sources: company_handbook.md].\\n    \", \"parameters\": {\"properties\": {\"query\": {\"type\": \"str\"}}}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool name: \", rag_query_engine.name)\n",
    "print(\"Tool signature: \", rag_query_engine.fn_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960a1e6-7b49-48fe-b4f6-b6e54d9c03b8",
   "metadata": {},
   "source": [
    "## Using the `agentic_patterns` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4cd7f2-cc28-4d8a-a227-8f1346a3b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_patterns.planning_pattern.react_agent import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2462f18-f4ed-494e-8676-454e883ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent(tools=[rag_query_engine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd048fc-1415-4ea1-a9b5-f488fc9a1ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "Thought: I need to query the knowledge base to find information on setting up the local dev environment for project Nexus.\n",
      "\u001b[32m\n",
      "Using Tool: rag_query_engine\n",
      "\u001b[32m\n",
      "Tool call dict: \n",
      "{'name': 'rag_query_engine', 'arguments': {'query': 'setup local dev environment project Nexus'}, 'id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "Tool result: \n",
      "To set up your local development environment for Project Nexus, follow these steps:\n",
      "\n",
      "1. Install Git by following the instructions on the official Git website.\n",
      "2. Download and install Docker Desktop from the official Docker website.\n",
      "3. Install Python 3.9, as it is required for Project Nexus due to a dependency issue.\n",
      "4. Install Node.js version 16 from the official Node.js website.\n",
      "5. Install the internal 'Nexus' library by running the command `pip install nexus-library`.\n",
      "\n",
      "Note: The Company Handbook recommends Python 3.8 for general projects, but Python 3.9 is necessary for Project Nexus. \n",
      "\n",
      "After setting up your local environment, you can request access to cloud resources using the `CloudProvisioner` tool. However, the command provided in the Project Nexus guide is outdated. Instead, use the command from the Company Handbook: `cprov request --role=developer --project=general`.\n",
      "\n",
      "[Sources: project_nexus_onboarding_guide.md, company_handbook.md, troubleshooting_local_setup.md]\n",
      "\u001b[34m\n",
      "Observations: {0: \"To set up your local development environment for Project Nexus, follow these steps:\\n\\n1. Install Git by following the instructions on the official Git website.\\n2. Download and install Docker Desktop from the official Docker website.\\n3. Install Python 3.9, as it is required for Project Nexus due to a dependency issue.\\n4. Install Node.js version 16 from the official Node.js website.\\n5. Install the internal 'Nexus' library by running the command `pip install nexus-library`.\\n\\nNote: The Company Handbook recommends Python 3.8 for general projects, but Python 3.9 is necessary for Project Nexus. \\n\\nAfter setting up your local environment, you can request access to cloud resources using the `CloudProvisioner` tool. However, the command provided in the Project Nexus guide is outdated. Instead, use the command from the Company Handbook: `cprov request --role=developer --project=general`.\\n\\n[Sources: project_nexus_onboarding_guide.md, company_handbook.md, troubleshooting_local_setup.md]\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To set up your local development environment for Project Nexus, follow these steps:\\n1. Install Git by following the instructions on the official Git website.\\n2. Download and install Docker Desktop from the official Docker website.\\n3. Install Python 3.9, as it is required for Project Nexus due to a dependency issue.\\n4. Install Node.js version 16 from the official Node.js website.\\n5. Install the internal 'Nexus' library by running the command `pip install nexus-library`.\\nNote: The Company Handbook recommends Python 3.8 for general projects, but Python 3.9 is necessary for Project Nexus.\\nAfter setting up your local environment, you can request access to cloud resources using the `CloudProvisioner` tool with the command `cprov request --role=developer --project=general`.\\n[Source: project_nexus_onboarding_guide.md, company_handbook.md, troubleshooting_local_setup.md]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(user_msg=\"How do I set up the local dev environment for project Nexus?\")\n",
    "#agent.run(user_msg=\"How do I set up the local dev environment for WordPress project?\")\n",
    "#agent.run(user_msg=\"Tell me the weather?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c78d6-760f-4161-b08b-7be9bf1fe010",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ReAct Agent - LlamaIndex Integration working as expected! ðŸš€ðŸš€ðŸš€ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-pattern-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
